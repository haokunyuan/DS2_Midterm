---
title: "final all model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(doParallel)
library(rpart)
library(rpart.plot)
cl <- makePSOCKcluster(16)
registerDoParallel(cl)
```

# Exploratory analysis 
```{r}
# read and process data
dept1_data = read_csv("../data/final_data_one_department.csv") %>% janitor::clean_names()
head(dept1_data)
# remove index 
dept1_data = dept1_data[-1]
# data type changed. 
dept1_data$is_holiday = as.factor(dept1_data$is_holiday)
dept1_data$type = as.factor(dept1_data$type)
dept1_data$store = as.factor(dept1_data$store)
#skimr::skim(dept1_data)
```
```{r}
# create a data frame for plot only 
dept1_data_plot = dept1_data
dept1_data_plot$is_holiday = as.numeric(dept1_data_plot$is_holiday)
dept1_data_plot$type = as.numeric(as.factor(dept1_data_plot$type))
dept1_data_plot$store = as.numeric(dept1_data_plot$store)
```

# distribtuion of response and corrlation 

```{r}
# figure 1 distribution of response 
dept1_data %>% 
  ggplot() + 
  geom_histogram(aes(x = weekly_sales))+
  theme_minimal()+
  ggtitle("Distribution of Weekly Sales")
```

```{r, fig.height = 4}
# set theme 
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

featurePlot(x = dept1_data_plot[,c(-3)],
            y = dept1_data$weekly_sales,
            span = .5, 
            plot = "scatter",
            type = c("p","smooth"),
            layout= c(4,3))


```

```{r}
plot_x = as.matrix(dept1_data_plot)
corrplot::corrplot(cor(plot_x))
```
# test and train split
```{r}
# seperate test and train 
smp_size <- floor(0.75 * nrow(dept1_data))
set.seed(123)
train_ind <- sample(seq_len(nrow(dept1_data)), size = smp_size)

train.data <-dept1_data[train_ind, ]
test.data  <- dept1_data[-train_ind, ]

train_matrix_x <- model.matrix(weekly_sales~.,train.data)[,-1]
train_matrix_y <- train.data$weekly_sales

test_matrix_x =  model.matrix(weekly_sales~.,test.data)[,-1]
test_matrix_y =  test.data$weekly_sales
```


# linear models 
```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
#linear 
set.seed(2)
lm.fit <- train(weekly_sales~.,
                data= train.data,
                method = "lm",
                trControl = ctrl1)
```

Ridge
```{r}
#ridge
set.seed(2)
ridge.fit <- train(weekly_sales~.,
                   data= train.data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(0, 20, length=50))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = function(x) log(x))
```



LASSO
```{r}
set.seed(2)
lasso.fit <- train(weekly_sales~.,
                   data= train.data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(2,10, length=100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = function(x) log(x))
```

Elastic net 
```{r}
set.seed(2)
enet.fit <- train(weekly_sales~.,
                   data= train.data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0,1,0.1), 
                                          lambda = exp(seq(0,15, length=100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
plot(enet.fit, xTrans = function(x) log(x))
```
PCR
```{r}
set.seed(2)
pcr.fit <- train(weekly_sales~.,
                 data= train.data,
                 method = "pcr",
                 tuneLength = 80,
                 trControl = ctrl1,
                 preProc = c("center", "scale") )
# # need to preprocess your data when using predict()
#trans <- preProcess(x, method = c("center", "scale"))
# predy2.pcr2 <- predict(pcr.fit$finalModel, 
#                        newdata = predict(trans,test.data),
#                        ncomp = pcr.fit$bestTune[[1]])
ggplot(pcr.fit, highlight = TRUE)+ theme_bw()
```
PLS
```{r}
set.seed(2)
pls.fit <-train(weekly_sales~.,
                data= train.data,
                method = "pls",
                tuneLength =55,
                trControl = ctrl1,
                scale = TRUE)

# predy2.pls2 <-predict(pls.fit$finalModel,
#                       newdata = test.data,
#                       ncomp = pls.fit$bestTune$ncomp)
ggplot(pls.fit, highlight = TRUE)+ theme_bw()
```

# gam 
```{r}
set.seed(2)
gam.fit <-train(weekly_sales~.,
                data= train.data,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp",
                                      select = c(TRUE,FALSE)),
                trControl = ctrl1)

gam.fit$finalModel
ggplot(gam.fit)+ theme_bw()
```
```{r}
set.seed(2)
gam.fit1 <-train(train_matrix_x,
                train_matrix_y,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp",
                                      select = c(TRUE,FALSE)),
                trControl = ctrl1)
```


#  regression tree and purne by lowest cross-validation errot

```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5,verboseIter = TRUE)
# Use CP from above pruned model, cp is complexity parameter
set.seed(1)
rpart.fit <- train(weekly_sales~., dept1_data,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-5,2, length = 100))),
                   trControl = ctrl)

ggplot(rpart.fit, highlight = TRUE)
                 
rpart.plot(rpart.fit$finalModel)
# rmse 
predicted_value = predict(rpart.fit, newdata = dept1_data[train.data,])
rmse = sqrt(mean((df$weekly_sales-predicted_value)^2))
                          
```

# Tree Based model 
bagging 
```{r}
set.seed(1)
bg.fit <- train(weekly_sales ~ ., 
                train.data,
                method = "ranger",
                tuneGrid = expand.grid(
                          mtry = 55, 
                          splitrule = "variance",
                          min.node.size = 10 ),
                importance = "permutation",
                scale.permutation.importance = "TRUE",
                trControl = ctrl)

# plot variable of importance 
# barplot(sort(ranger::importance(bg.fit$finalModel), decreasing = FALSE),
#         las = 2, horiz = TRUE, cex.names = 0.7)
```
Random forest 
```{r}
rf.grid <- expand.grid(mtry = c(52,53,54,55),
                       splitrule = "variance",
                       min.node.size = 10)
set.seed(1)
rf.fit <- train(weekly_sales~., train.data,
                method = "ranger",
                tuneGrid = rf.grid,
                importance = "permutation",
                scale.permutation.importance = "TRUE",
                trControl = ctrl)

#rf.fit$bestTune
ggplot(rf.fit, highlight = TRUE)
```

```{r}
barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7)
#varImp(rf.fit)
```


regression tree 
```{r}
# Use CP from above pruned model, cp is complexity parameter
set.seed(1)
rpart.fit <- train(weekly_sales~., train.data,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-12,4, length = 100))),
                   trControl = ctrl)

plot(rpart.fit, highlight = TRUE, xTrans = function(x) log(x))
                 
```
# Boosting
```{r}
gbm.grid <- expand.grid(n.trees = (15:20)*1000,
                        interaction.depth = c(7,9),
                        shrinkage = c(0.05,0.025,0.01),
                        n.minobsinnode = c(100))
set.seed(1)
gbm.fit <- train(weekly_sales~., train.data,
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = TRUE)

ggplot(gbm.fit, highlight = TRUE)
```
```{r}
varImp(gbm.fit)
```



Model Selection
```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit,
                         enet = enet.fit,
                         lm = lm.fit,
                         pcr = pcr.fit,
                         pls = pls.fit,
                         gam = gam.fit,
                         bagging = bg.fit,
                         rf=rf.fit,
                         regression_tree = rpart.fit,
                         boosting=gbm.fit))

bwplot(resamp, metric = "RMSE")
```

# test set performance 

```{r}
lm_pred = predict(lm.fit, newdata = test.data)
lasso_pred = predict(lasso.fit, newdata = test.data)
ridge_pred = predict(ridge.fit, newdata = test.data)
enet_pred = predict(enet.fit, newdata = test.data)
pcr_pred = predict(pcr.fit, newdata = test.data)
pls_pred = predict(pls.fit, newdata = test.data)
#gam_pred = predict(gam.fit, newdata = test.data)


rpart_pred = predict(rpart.fit, newdata = test.data)
#sm_reg_tree = predict(rpart.fit.trim, newdata = test.data)
bagging = predict(bg.fit, newdata = test.data)
rf = predict(rf.fit, newdata = test.data)
gbm = predict(gbm.fit, newdata = test.data)

postResample(pred = lm_pred, obs = test.data$weekly_sales)
postResample(pred = lasso_pred, obs = test.data$weekly_sales)
postResample(pred = ridge_pred, obs = test.data$weekly_sales)
postResample(pred = enet_pred, obs = test.data$weekly_sales)
postResample(pred = pcr_pred, obs = test.data$weekly_sales)
postResample(pred = pls_pred, obs = test.data$weekly_sales)

postResample(pred = rpart_pred, obs = test.data$weekly_sales)
#postResample(pred = sm_reg_tree, obs = test.data$weekly_sales)
postResample(pred = bagging, obs = test.data$weekly_sales)
postResample(pred = rf, obs = test.data$weekly_sales)
postResample(pred = gbm, obs = test.data$weekly_sales)



test_perfm = rbind(postResample(pred = lm_pred, obs = test.data$weekly_sales),
postResample(pred = lasso_pred, obs = test.data$weekly_sales),
postResample(pred = ridge_pred, obs = test.data$weekly_sales),
postResample(pred = enet_pred, obs = test.data$weekly_sales),
postResample(pred = pcr_pred, obs = test.data$weekly_sales),
postResample(pred = pls_pred, obs = test.data$weekly_sales),

postResample(pred = rpart_pred, obs = test.data$weekly_sales),
#postResample(pred = sm_reg_tree, obs = test.data$weekly_sales),
postResample(pred = bagging, obs = test.data$weekly_sales),
postResample(pred = rf, obs = test.data$weekly_sales),
postResample(pred = gbm, obs = test.data$weekly_sales))

rownames(test_perfm) = c("lm","lasso","ridge","enet","pcr","pls","rpart","bagging","rf","gbm")
round(test_perfm,1)

trans <- preProcess(x, method = c("scale"))
predy2.pls2 <-predict(pls.fit$finalModel,
                      newdata =test_matrix_x,
                      ncomp = pls.fit$bestTune$ncomp)
mean((predy2.pls2-test_matrix_y)^2) %>% sqrt()


#
# predy2.pcr2 <- predict(pcr.fit$finalModel, 
#                        newdata = predict(trans,test.data),
#                        ncomp = pcr.fit$bestTune[[1]])
```

































